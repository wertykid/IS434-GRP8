{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a76c666",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Instagram comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1751c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14281d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ed7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print sentiments\n",
    "# of the sentence.\n",
    "def sentiment_scores(sentence):\n",
    "# Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "# polarity_scores method of SentimentIntensityAnalyzer\n",
    "# oject gives a sentiment dictionary.\n",
    "# which contains pos, neg, neu, and compound scores.\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        result = \"Positive\"\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        result = \"Negative\"\n",
    "    else :\n",
    "        result = \"Neutral\"\n",
    "    \n",
    "#     print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4928bb0",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on comments of posts by santacruzbicycles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ba5cc",
   "metadata": {},
   "source": [
    "### Reading json file from scraped datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "comment_files = []\n",
    "for name in glob.glob('santacruzbicycles/*comments.json'):\n",
    "    print(name)\n",
    "    comment_files.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481035ba",
   "metadata": {},
   "source": [
    "### Sentiment Analysis on comments of each post per account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e23fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_results = pd.DataFrame(columns=['file_name','post_id','comment',\n",
    "                                        'Negative', 'Neutral', 'Positive', 'Compound', 'Sentiment'])\n",
    "\n",
    "\n",
    "for i in range(len(comment_files)):\n",
    "    # Opening JSON file\n",
    "    f = open(comment_files[i])\n",
    " \n",
    "    # returns JSON object as\n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    file_name = comment_files[i].split('/')[1] # get the json file name to check \n",
    "    print(\"json filename:\", file_name)\n",
    "    \n",
    "    # Closing file\n",
    "    f.close()\n",
    "    \n",
    "    for comment in data:\n",
    "        og_sentence = comment['text']\n",
    "        \n",
    "        filter(lambda x:x[0]!='@', og_sentence.split())\n",
    "        sentence = \" \".join(filter(lambda x:x[0]!='@', og_sentence.split()))\n",
    "        \n",
    "        print(\"original comment: \" + og_sentence)\n",
    "        print(\"cleaned comment: \" + sentence)\n",
    "        \n",
    "        sentiment_dict = SentimentIntensityAnalyzer().polarity_scores(sentence)\n",
    "        comment_results = comment_results.append({'file_name':file_name,\n",
    "                                                'post_id': comment['id'], \n",
    "                                                'comment': og_sentence,\n",
    "                                                'Negative': sentiment_dict['neg'],\n",
    "                                                'Neutral':  sentiment_dict['neu'],\n",
    "                                                'Positive': sentiment_dict['pos'],\n",
    "                                                'Compound': sentiment_dict['compound'],\n",
    "                                                'Sentiment': sentiment_scores(sentence)}, ignore_index=True)\n",
    "        print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "        print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "        print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "        print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    "        print(\"Sentence Overall Rated As\", sentiment_scores(sentence))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5c80c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comment_results.to_csv('santacruzbicycles_sentiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "santacruzbicycles_sentiments = pd.read_csv('santacruzbicycles_sentiments.csv')\n",
    "del santacruzbicycles_sentiments['Unnamed: 0']\n",
    "santacruzbicycles_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "santacruzbicycles_sentiments['date'] = santacruzbicycles_sentiments['file_name'].apply(lambda x: x[:10])\n",
    "santacruzbicycles_sentiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "santacruzbicycles_sentiments.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1883a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "santacruzbicycles_date = santacruzbicycles_sentiments.groupby('date').mean('Compound')\n",
    "santacruzbicycles_date = santacruzbicycles_date.drop(['post_id'], axis=1)\n",
    "santacruzbicycles_date.reset_index(inplace=True)\n",
    "santacruzbicycles_date = santacruzbicycles_date.rename(columns = {'index':'date'})\n",
    "santacruzbicycles_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc54e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "santacruzbicycles_date.to_csv('santacruzbicycles_date.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf53e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate WordCloud \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \" \".join(lemmatizer.lemmatize(com.lower()) for com in santacruzbicycles_sentiments.comment.astype(str))\n",
    "print (\"There are {} words in the combination of all comments in unsprungsg account.\".format(len(text)))\n",
    "\n",
    "# Create stopword list:\n",
    "# remove words that we want to exclude\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"santacruzbicycle\",\"santacruzbicycles\",\"santa\",\"cruz\",\"bike\"])\n",
    "\n",
    "# Generate a word cloud image\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", width=800, height=400).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.figure( figsize=(40,20))\n",
    "plt.tight_layout(pad=0)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9957e7e",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on comments of posts by unsprungsg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6911515",
   "metadata": {},
   "source": [
    "### Reading json file from scraped datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "comment_files = []\n",
    "for name in glob.glob('unsprungsg/*comments.json'):\n",
    "    print(name)\n",
    "    comment_files.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ebcf37",
   "metadata": {},
   "source": [
    "### Sentiment Analysis on comments of each post per account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a5b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsprungsg_results = pd.DataFrame(columns=['file_name','post_id','comment',\n",
    "                                        'Negative', 'Neutral', 'Positive', 'Compound', 'Sentiment'])\n",
    "\n",
    "\n",
    "for i in range(len(comment_files)):\n",
    "    # Opening JSON file\n",
    "    f = open(comment_files[i])\n",
    " \n",
    "    # returns JSON object as\n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    file_name = comment_files[i].split('/')[1] # get the json file name to check \n",
    "    print(\"json filename:\", file_name)\n",
    "    \n",
    "    # Closing file\n",
    "    f.close()\n",
    "    \n",
    "    for comment in data:\n",
    "        og_sentence = comment['text']\n",
    "        \n",
    "        filter(lambda x:x[0]!='@', og_sentence.split())\n",
    "        sentence = \" \".join(filter(lambda x:x[0]!='@', og_sentence.split()))\n",
    "        \n",
    "        print(\"original comment: \" + og_sentence)\n",
    "        print(\"cleaned comment: \" + sentence)\n",
    "        \n",
    "        sentiment_dict = SentimentIntensityAnalyzer().polarity_scores(sentence)\n",
    "        unsprungsg_results = unsprungsg_results.append({'file_name':file_name,\n",
    "                                                'post_id': comment['id'], \n",
    "                                                'comment': og_sentence,\n",
    "                                                'Negative': sentiment_dict['neg'],\n",
    "                                                'Neutral':  sentiment_dict['neu'],\n",
    "                                                'Positive': sentiment_dict['pos'],\n",
    "                                                'Compound': sentiment_dict['compound'],\n",
    "                                                'Sentiment': sentiment_scores(sentence)}, ignore_index=True)\n",
    "        print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "        print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "        print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "        print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    "        print(\"Sentence Overall Rated As\", sentiment_scores(sentence))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93789b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsprungsg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsprungsg_results.to_csv('unsprungsg_sentiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5738331",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsprungsg_sentiments = pd.read_csv('unsprungsg_sentiments.csv')\n",
    "del unsprungsg_sentiments['Unnamed: 0']\n",
    "unsprungsg_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d53c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsprungsg_sentiments['date'] = unsprungsg_sentiments['file_name'].apply(lambda x: x[:10])\n",
    "unsprungsg_date = unsprungsg_sentiments.groupby('date').mean('Compound')\n",
    "unsprungsg_date = unsprungsg_date.drop(['post_id'], axis=1)\n",
    "unsprungsg_date.reset_index(inplace=True)\n",
    "unsprungsg_date = unsprungsg_date.rename(columns = {'index':'date'})\n",
    "unsprungsg_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeef373",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsprungsg_date.to_csv('unsprungsg_date.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4496b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate WordCloud \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \" \".join(lemmatizer.lemmatize(com) for com in unsprungsg_sentiments.comment.astype(str))\n",
    "print (\"There are {} words in the combination of all comments in unsprungsg account.\".format(len(text)))\n",
    "\n",
    "# Create stopword list:\n",
    "# remove words that we want to exclude\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"unsprungsg\",\"bike\"])\n",
    "\n",
    "# Generate a word cloud image\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", width=800, height=400).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.figure( figsize=(40,20))\n",
    "plt.tight_layout(pad=0)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3094071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6333f334",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on comments of posts by forbiddenbikecompany\n",
    "\n",
    "### Reading json file from scraped datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "comment_files = []\n",
    "for name in glob.glob('forbiddenbikecompany/forbiddenbikecompany/*comments.json'):\n",
    "    print(name)\n",
    "    comment_files.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d46d7",
   "metadata": {},
   "source": [
    "### Sentiment Analysis on comments of each post per account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcefeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "forbiddenbikecompany_results = pd.DataFrame(columns=['file_name','post_id','comment',\n",
    "                                        'Negative', 'Neutral', 'Positive', 'Compound', 'Sentiment'])\n",
    "\n",
    "\n",
    "for i in range(len(comment_files)):\n",
    "    # Opening JSON file\n",
    "    f = open(comment_files[i])\n",
    " \n",
    "    # returns JSON object as\n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    file_name = comment_files[i].split('/')[2] # get the json file name to check \n",
    "    print(\"json filename:\", file_name)\n",
    "    \n",
    "    # Closing file\n",
    "    f.close()\n",
    "    \n",
    "    for comment in data:\n",
    "        og_sentence = comment['text']\n",
    "        \n",
    "        filter(lambda x:x[0]!='@', og_sentence.split())\n",
    "        sentence = \" \".join(filter(lambda x:x[0]!='@', og_sentence.split()))\n",
    "        \n",
    "        print(\"original comment: \" + og_sentence)\n",
    "        print(\"cleaned comment: \" + sentence)\n",
    "        \n",
    "        sentiment_dict = SentimentIntensityAnalyzer().polarity_scores(sentence)\n",
    "        forbiddenbikecompany_results = forbiddenbikecompany_results.append({'file_name':file_name,\n",
    "                                                'post_id': comment['id'], \n",
    "                                                'comment': og_sentence,\n",
    "                                                'Negative': sentiment_dict['neg'],\n",
    "                                                'Neutral':  sentiment_dict['neu'],\n",
    "                                                'Positive': sentiment_dict['pos'],\n",
    "                                                'Compound': sentiment_dict['compound'],\n",
    "                                                'Sentiment': sentiment_scores(sentence)}, ignore_index=True)\n",
    "        print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "        print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "        print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "        print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    "        print(\"Sentence Overall Rated As\", sentiment_scores(sentence))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072dc29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forbiddenbikecompany_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372809d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "forbiddenbikecompany_results.to_csv('forbiddenbikecompany_sentiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa96219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forbiddenbikecompany_sentiments = pd.read_csv('forbiddenbikecompany_sentiments.csv')\n",
    "del forbiddenbikecompany_sentiments['Unnamed: 0']\n",
    "forbiddenbikecompany_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments with highest sentiment score\n",
    "forbiddenbikecompany_sentiments.nlargest(10,['Compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "forbiddenbike_date = forbiddenbikecompany_sentiments.groupby('file_name').mean('Compound')\n",
    "forbiddenbike_date = forbiddenbike_date.drop(['post_id'], axis=1)\n",
    "forbiddenbike_date.reset_index(inplace=True)\n",
    "forbiddenbike_date = forbiddenbike_date.rename(columns = {'index':'date'})\n",
    "forbiddenbike_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9961991",
   "metadata": {},
   "outputs": [],
   "source": [
    "forbiddenbikecompany_sentiments['date'] = forbiddenbikecompany_sentiments['file_name'].apply(lambda x: x[:10])\n",
    "forbiddenbike_date = forbiddenbikecompany_sentiments.groupby('date').mean('Compound')\n",
    "forbiddenbike_date = forbiddenbike_date.drop(['post_id'], axis=1)\n",
    "forbiddenbike_date.reset_index(inplace=True)\n",
    "forbiddenbike_date = forbiddenbike_date.rename(columns = {'index':'date'})\n",
    "forbiddenbike_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446bb3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate WordCloud \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \" \".join(lemmatizer.lemmatize(com) for com in forbiddenbikecompany_sentiments.comment.astype(str))\n",
    "print (\"There are {} words in the combination of all comments in forbiddenbikecompany account.\".format(len(text)))\n",
    "\n",
    "# Create stopword list:\n",
    "# remove words that we want to exclude\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"forbiddenbikecompany\",\"bike\"])\n",
    "\n",
    "# Generate a word cloud image\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", width=800, height=400).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.figure( figsize=(40,20))\n",
    "plt.tight_layout(pad=0)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e9276",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on comments of posts by yeticycles\n",
    "### Reading json file from scraped datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041391f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "comment_files = []\n",
    "for name in glob.glob('yeticycles/yeticycles/*comments.json'):\n",
    "    print(name)\n",
    "    comment_files.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d41c16",
   "metadata": {},
   "source": [
    "### Sentiment Analysis on comments of each post per account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c54a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeticycles_results = pd.DataFrame(columns=['file_name','post_id','comment',\n",
    "                                        'Negative', 'Neutral', 'Positive', 'Compound', 'Sentiment'])\n",
    "\n",
    "\n",
    "for i in range(len(comment_files)):\n",
    "    # Opening JSON file\n",
    "    f = open(comment_files[i])\n",
    " \n",
    "    # returns JSON object as\n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "    file_name = comment_files[i].split('/')[2] # get the json file name to check \n",
    "    print(\"json filename:\", file_name)\n",
    "    \n",
    "    # Closing file\n",
    "    f.close()\n",
    "    \n",
    "    for comment in data:\n",
    "        og_sentence = comment['text']\n",
    "        \n",
    "        filter(lambda x:x[0]!='@', og_sentence.split())\n",
    "        sentence = \" \".join(filter(lambda x:x[0]!='@', og_sentence.split()))\n",
    "        \n",
    "        print(\"original comment: \" + og_sentence)\n",
    "        print(\"cleaned comment: \" + sentence)\n",
    "        \n",
    "        sentiment_dict = SentimentIntensityAnalyzer().polarity_scores(sentence)\n",
    "        yeticycles_results = yeticycles_results.append({'file_name':file_name,\n",
    "                                                'post_id': comment['id'], \n",
    "                                                'comment': og_sentence,\n",
    "                                                'Negative': sentiment_dict['neg'],\n",
    "                                                'Neutral':  sentiment_dict['neu'],\n",
    "                                                'Positive': sentiment_dict['pos'],\n",
    "                                                'Compound': sentiment_dict['compound'],\n",
    "                                                'Sentiment': sentiment_scores(sentence)}, ignore_index=True)\n",
    "        print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "        print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "        print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "        print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    "        print(\"Sentence Overall Rated As\", sentiment_scores(sentence))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0bc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeticycles_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeticycles_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b958df",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeticycles_results.to_csv('yeticycles_sentiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeticycles_sentiments = pd.read_csv('yeticycles_sentiments.csv')\n",
    "del yeticycles_sentiments['Unnamed: 0']\n",
    "yeticycles_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeticycles_sentiments['date'] = yeticycles_sentiments['file_name'].apply(lambda x: x[:10])\n",
    "yeticycles_date = yeticycles_sentiments.groupby('date').mean('Compound')\n",
    "yeticycles_date = yeticycles_date.drop(['post_id'], axis=1)\n",
    "yeticycles_date.reset_index(inplace=True)\n",
    "yeticycles_date = yeticycles_date.rename(columns = {'index':'date'})\n",
    "yeticycles_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9a4b7",
   "metadata": {},
   "source": [
    "## Wordcloud for comments on yeticycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b42853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate WordCloud \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \" \".join(lemmatizer.lemmatize(com) for com in yeticycles_sentiments.comment.astype(str))\n",
    "print (\"There are {} words in the combination of all comments in yeticycles account.\".format(len(text)))\n",
    "\n",
    "# Create stopword list:\n",
    "# remove words that we want to exclude\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"yeti\", \"yeticycles\",\"bike\"])\n",
    "\n",
    "# Generate a word cloud image\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", width=800, height=400).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.figure( figsize=(40,20))\n",
    "plt.tight_layout(pad=0)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53cbdae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03606a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeticycles_date['brand'] = 'yeticycles'\n",
    "forbiddenbike_date['brand'] = 'forbiddenbike'\n",
    "unsprungsg_date['brand'] = 'unsprungsg'\n",
    "santacruzbicycles_date['brand'] = 'santacruzbicycles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "forbiddenbike_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9819012",
   "metadata": {},
   "outputs": [],
   "source": [
    "yeticycles_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a12620",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [santacruzbicycles_date, unsprungsg_date, forbiddenbike_date, yeticycles_date]\n",
    "\n",
    "sentiments_date = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49399f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_date.to_csv('sentiments_date.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
